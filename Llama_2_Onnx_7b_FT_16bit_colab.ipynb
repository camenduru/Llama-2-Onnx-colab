{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/Llama-2-Onnx-colab/blob/main/Llama_2_Onnx_7b_FT_16bit_colab.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VCFOzsQSHbjM"
      },
      "outputs": [],
      "source": [
        "# @title Install\n",
        "%cd /content\n",
        "!pip install -q onnxruntime-gpu sentencepiece\n",
        "!git clone https://github.com/microsoft/Llama-2-Onnx.git\n",
        "!git clone https://huggingface.co/4bit/7B_FT_float16\n",
        "\n",
        "# !apt-get -y install -qq aria2\n",
        "# !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/7B_FT_float16/resolve/main/embeddings.pth -d /content/7B_FT_float16 -o embeddings.pth\n",
        "# !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/7B_FT_float16/resolve/main/tokenizer.model -d /content/7B_FT_float16 -o tokenizer.model\n",
        "# links = [\"Constant_69_attr__value\", \"Constant_77_attr__value\", \"LlamaV2_7B_FT_float16.onnx\", \"embeddings.data\", \"onnx__MatMul_20670\", \"onnx__MatMul_20671\", \"onnx__MatMul_20672\", \"onnx__MatMul_20692\", \"onnx__MatMul_20693\", \"onnx__MatMul_20694\", \"onnx__MatMul_20695\", \"onnx__MatMul_20696\", \"onnx__MatMul_20697\", \"onnx__MatMul_20698\", \"onnx__MatMul_20718\", \"onnx__MatMul_20719\", \"onnx__MatMul_20720\", \"onnx__MatMul_20721\", \"onnx__MatMul_20722\", \"onnx__MatMul_20723\", \"onnx__MatMul_20724\", \"onnx__MatMul_20744\", \"onnx__MatMul_20745\", \"onnx__MatMul_20746\", \"onnx__MatMul_20747\", \"onnx__MatMul_20748\", \"onnx__MatMul_20749\", \"onnx__MatMul_20750\", \"onnx__MatMul_20770\", \"onnx__MatMul_20771\", \"onnx__MatMul_20772\", \"onnx__MatMul_20773\", \"onnx__MatMul_20774\", \"onnx__MatMul_20775\", \"onnx__MatMul_20776\", \"onnx__MatMul_20796\", \"onnx__MatMul_20797\", \"onnx__MatMul_20798\", \"onnx__MatMul_20799\", \"onnx__MatMul_20800\", \"onnx__MatMul_20801\", \"onnx__MatMul_20802\", \"onnx__MatMul_20822\", \"onnx__MatMul_20823\", \"onnx__MatMul_20824\", \"onnx__MatMul_20825\", \"onnx__MatMul_20826\", \"onnx__MatMul_20827\", \"onnx__MatMul_20828\", \"onnx__MatMul_20848\", \"onnx__MatMul_20849\", \"onnx__MatMul_20850\", \"onnx__MatMul_20851\", \"onnx__MatMul_20852\", \"onnx__MatMul_20853\", \"onnx__MatMul_20854\", \"onnx__MatMul_20874\", \"onnx__MatMul_20875\", \"onnx__MatMul_20876\", \"onnx__MatMul_20877\", \"onnx__MatMul_20878\", \"onnx__MatMul_20879\", \"onnx__MatMul_20880\", \"onnx__MatMul_20900\", \"onnx__MatMul_20901\", \"onnx__MatMul_20902\", \"onnx__MatMul_20903\", \"onnx__MatMul_20904\", \"onnx__MatMul_20905\", \"onnx__MatMul_20906\", \"onnx__MatMul_20926\", \"onnx__MatMul_20927\", \"onnx__MatMul_20928\", \"onnx__MatMul_20929\", \"onnx__MatMul_20930\", \"onnx__MatMul_20931\", \"onnx__MatMul_20932\", \"onnx__MatMul_20952\", \"onnx__MatMul_20953\", \"onnx__MatMul_20954\", \"onnx__MatMul_20955\", \"onnx__MatMul_20956\", \"onnx__MatMul_20957\", \"onnx__MatMul_20958\", \"onnx__MatMul_20978\", \"onnx__MatMul_20979\", \"onnx__MatMul_20980\", \"onnx__MatMul_20981\", \"onnx__MatMul_20982\", \"onnx__MatMul_20983\", \"onnx__MatMul_20984\", \"onnx__MatMul_21004\", \"onnx__MatMul_21005\", \"onnx__MatMul_21006\", \"onnx__MatMul_21007\", \"onnx__MatMul_21008\", \"onnx__MatMul_21009\", \"onnx__MatMul_21010\", \"onnx__MatMul_21030\", \"onnx__MatMul_21031\", \"onnx__MatMul_21032\", \"onnx__MatMul_21033\", \"onnx__MatMul_21034\", \"onnx__MatMul_21035\", \"onnx__MatMul_21036\", \"onnx__MatMul_21056\", \"onnx__MatMul_21057\", \"onnx__MatMul_21058\", \"onnx__MatMul_21059\", \"onnx__MatMul_21060\", \"onnx__MatMul_21061\", \"onnx__MatMul_21062\", \"onnx__MatMul_21082\", \"onnx__MatMul_21083\", \"onnx__MatMul_21084\", \"onnx__MatMul_21085\", \"onnx__MatMul_21086\", \"onnx__MatMul_21087\", \"onnx__MatMul_21088\", \"onnx__MatMul_21108\", \"onnx__MatMul_21109\", \"onnx__MatMul_21110\", \"onnx__MatMul_21111\", \"onnx__MatMul_21112\", \"onnx__MatMul_21113\", \"onnx__MatMul_21114\", \"onnx__MatMul_21134\", \"onnx__MatMul_21135\", \"onnx__MatMul_21136\", \"onnx__MatMul_21137\", \"onnx__MatMul_21138\", \"onnx__MatMul_21139\", \"onnx__MatMul_21140\", \"onnx__MatMul_21160\", \"onnx__MatMul_21161\", \"onnx__MatMul_21162\", \"onnx__MatMul_21163\", \"onnx__MatMul_21164\", \"onnx__MatMul_21165\", \"onnx__MatMul_21166\", \"onnx__MatMul_21186\", \"onnx__MatMul_21187\", \"onnx__MatMul_21188\", \"onnx__MatMul_21189\", \"onnx__MatMul_21190\", \"onnx__MatMul_21191\", \"onnx__MatMul_21192\", \"onnx__MatMul_21212\", \"onnx__MatMul_21213\", \"onnx__MatMul_21214\", \"onnx__MatMul_21215\", \"onnx__MatMul_21216\", \"onnx__MatMul_21217\", \"onnx__MatMul_21218\", \"onnx__MatMul_21238\", \"onnx__MatMul_21239\", \"onnx__MatMul_21240\", \"onnx__MatMul_21241\", \"onnx__MatMul_21242\", \"onnx__MatMul_21243\", \"onnx__MatMul_21244\", \"onnx__MatMul_21264\", \"onnx__MatMul_21265\", \"onnx__MatMul_21266\", \"onnx__MatMul_21267\", \"onnx__MatMul_21268\", \"onnx__MatMul_21269\", \"onnx__MatMul_21270\", \"onnx__MatMul_21290\", \"onnx__MatMul_21291\", \"onnx__MatMul_21292\", \"onnx__MatMul_21293\", \"onnx__MatMul_21294\", \"onnx__MatMul_21295\", \"onnx__MatMul_21296\", \"onnx__MatMul_21316\", \"onnx__MatMul_21317\", \"onnx__MatMul_21318\", \"onnx__MatMul_21319\", \"onnx__MatMul_21320\", \"onnx__MatMul_21321\", \"onnx__MatMul_21322\", \"onnx__MatMul_21342\", \"onnx__MatMul_21343\", \"onnx__MatMul_21344\", \"onnx__MatMul_21345\", \"onnx__MatMul_21346\", \"onnx__MatMul_21347\", \"onnx__MatMul_21348\", \"onnx__MatMul_21368\", \"onnx__MatMul_21369\", \"onnx__MatMul_21370\", \"onnx__MatMul_21371\", \"onnx__MatMul_21372\", \"onnx__MatMul_21373\", \"onnx__MatMul_21374\", \"onnx__MatMul_21394\", \"onnx__MatMul_21395\", \"onnx__MatMul_21396\", \"onnx__MatMul_21397\", \"onnx__MatMul_21398\", \"onnx__MatMul_21399\", \"onnx__MatMul_21400\", \"onnx__MatMul_21420\", \"onnx__MatMul_21421\", \"onnx__MatMul_21422\", \"onnx__MatMul_21423\", \"onnx__MatMul_21424\", \"onnx__MatMul_21425\", \"onnx__MatMul_21426\", \"onnx__MatMul_21446\", \"onnx__MatMul_21447\", \"onnx__MatMul_21448\", \"onnx__MatMul_21449\", \"onnx__MatMul_21450\", \"onnx__MatMul_21451\", \"onnx__MatMul_21452\", \"onnx__MatMul_21472\", \"onnx__MatMul_21473\", \"onnx__MatMul_21474\", \"onnx__MatMul_21475\", \"onnx__MatMul_21476\", \"onnx__MatMul_21477\", \"onnx__MatMul_21478\", \"onnx__MatMul_21498\", \"onnx__MatMul_21499\", \"onnx__MatMul_21500\", \"onnx__MatMul_21501\", \"onnx__MatMul_21502\", \"transformer.block_list.0.attn_norm.weight\", \"transformer.block_list.0.proj_norm.weight\", \"transformer.block_list.1.attn_norm.weight\", \"transformer.block_list.1.proj_norm.weight\", \"transformer.block_list.10.attn_norm.weight\", \"transformer.block_list.10.proj_norm.weight\", \"transformer.block_list.11.attn_norm.weight\", \"transformer.block_list.11.proj_norm.weight\", \"transformer.block_list.12.attn_norm.weight\", \"transformer.block_list.12.proj_norm.weight\", \"transformer.block_list.13.attn_norm.weight\", \"transformer.block_list.13.proj_norm.weight\", \"transformer.block_list.14.attn_norm.weight\", \"transformer.block_list.14.proj_norm.weight\", \"transformer.block_list.15.attn_norm.weight\", \"transformer.block_list.15.proj_norm.weight\", \"transformer.block_list.16.attn_norm.weight\", \"transformer.block_list.16.proj_norm.weight\", \"transformer.block_list.17.attn_norm.weight\", \"transformer.block_list.17.proj_norm.weight\", \"transformer.block_list.18.attn_norm.weight\", \"transformer.block_list.18.proj_norm.weight\", \"transformer.block_list.19.attn_norm.weight\", \"transformer.block_list.19.proj_norm.weight\", \"transformer.block_list.2.attn_norm.weight\", \"transformer.block_list.2.proj_norm.weight\", \"transformer.block_list.20.attn_norm.weight\", \"transformer.block_list.20.proj_norm.weight\", \"transformer.block_list.21.attn_norm.weight\", \"transformer.block_list.21.proj_norm.weight\", \"transformer.block_list.22.attn_norm.weight\", \"transformer.block_list.22.proj_norm.weight\", \"transformer.block_list.23.attn_norm.weight\", \"transformer.block_list.23.proj_norm.weight\", \"transformer.block_list.24.attn_norm.weight\", \"transformer.block_list.24.proj_norm.weight\", \"transformer.block_list.25.attn_norm.weight\", \"transformer.block_list.25.proj_norm.weight\", \"transformer.block_list.26.attn_norm.weight\", \"transformer.block_list.26.proj_norm.weight\", \"transformer.block_list.27.attn_norm.weight\", \"transformer.block_list.27.proj_norm.weight\", \"transformer.block_list.28.attn_norm.weight\", \"transformer.block_list.28.proj_norm.weight\", \"transformer.block_list.29.attn_norm.weight\", \"transformer.block_list.29.proj_norm.weight\", \"transformer.block_list.3.attn_norm.weight\", \"transformer.block_list.3.proj_norm.weight\", \"transformer.block_list.30.attn_norm.weight\", \"transformer.block_list.30.proj_norm.weight\", \"transformer.block_list.31.attn_norm.weight\", \"transformer.block_list.31.proj_norm.weight\", \"transformer.block_list.4.attn_norm.weight\", \"transformer.block_list.4.proj_norm.weight\", \"transformer.block_list.5.attn_norm.weight\", \"transformer.block_list.5.proj_norm.weight\", \"transformer.block_list.6.attn_norm.weight\", \"transformer.block_list.6.proj_norm.weight\", \"transformer.block_list.7.attn_norm.weight\", \"transformer.block_list.7.proj_norm.weight\", \"transformer.block_list.8.attn_norm.weight\", \"transformer.block_list.8.proj_norm.weight\", \"transformer.block_list.9.attn_norm.weight\", \"transformer.block_list.9.proj_norm.weight\", \"transformer.layer_norm.weight\"]\n",
        "# for link in links:\n",
        "#   !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/7B_FT_float16/resolve/main/ONNX/{link} -d /content/7B_FT_float16/ONNX -o {link}\n",
        "\n",
        "# %cd /content/Llama-2-Onnx\n",
        "# !python MinimumExample/Example_ONNX_LlamaV2.py \\\n",
        "# --onnx_file /content/7B_FT_float16/ONNX/LlamaV2_7B_FT_float16.onnx \\\n",
        "# --embedding_file /content/7B_FT_float16/embeddings.pth \\\n",
        "# --tokenizer_path /content/7B_FT_float16/tokenizer.model \\\n",
        "# --prompt \"What is the lightest element?\"\n",
        "\n",
        "# This program will run the ONNX version of the LlamaV2 model.\n",
        "import torch\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "from typing import List\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, model_path: str):\n",
        "        # reload tokenizer\n",
        "        assert os.path.isfile(model_path), model_path\n",
        "        self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
        "\n",
        "        # BOS / EOS token IDs\n",
        "        self.n_words: int = self.sp_model.vocab_size()\n",
        "        self.bos_id: int = self.sp_model.bos_id()\n",
        "        self.eos_id: int = self.sp_model.eos_id()\n",
        "        self.pad_id: int = self.sp_model.pad_id()\n",
        "\n",
        "        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n",
        "\n",
        "    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
        "        assert type(s) is str\n",
        "        t = self.sp_model.encode(s)\n",
        "        if bos:\n",
        "            t = [self.bos_id] + t\n",
        "        if eos:\n",
        "            t = t + [self.eos_id]\n",
        "        return t\n",
        "\n",
        "    def decode(self, t: List[int]) -> str:\n",
        "        return self.sp_model.decode(t)\n",
        "\n",
        "options = onnxruntime.SessionOptions()\n",
        "llm_session = onnxruntime.InferenceSession('/content/7B_FT_float16/ONNX/LlamaV2_7B_FT_float16.onnx', sess_options=options,\n",
        "    providers=[\n",
        "        \"DmlExecutionProvider\",\n",
        "        \"CUDAExecutionProvider\",\n",
        "        \"CPUExecutionProvider\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# get the data type used by the model\n",
        "data_type_str = llm_session.get_inputs()[0].type\n",
        "if data_type_str == \"tensor(float16)\":\n",
        "    data_type = np.float16\n",
        "elif data_type_str == \"tensor(float32)\" or data_type_str == \"tensor(float)\":\n",
        "    data_type = np.float32\n",
        "else:\n",
        "    raise Exception(f\"Unknown data type {data_type_str}\")\n",
        "\n",
        "# Get the relevant shapes so we can create the inputs\n",
        "for inputs_meta in llm_session._inputs_meta:\n",
        "    if inputs_meta.name == \"x\":\n",
        "        x_shape = inputs_meta.shape\n",
        "    elif inputs_meta.name == \"attn_mask\":\n",
        "        attn_mask_shape = inputs_meta.shape\n",
        "    elif inputs_meta.name == \"k_cache\":\n",
        "        k_cache_shape = inputs_meta.shape\n",
        "\n",
        "hidden_size = x_shape[2]\n",
        "max_seq_len = attn_mask_shape[1]\n",
        "n_layers = k_cache_shape[1]\n",
        "n_heads = k_cache_shape[3]\n",
        "\n",
        "# Initialize the tokenizer and produce the initial tokens.\n",
        "tokenizer = Tokenizer(model_path='/content/7B_FT_float16/tokenizer.model')\n",
        "\n",
        "# create the embedding layer.\n",
        "embedding_layer = torch.nn.Embedding(tokenizer.n_words, hidden_size)\n",
        "embedding_layer.load_state_dict(torch.load('/content/7B_FT_float16/embeddings.pth'))\n",
        "embedding_layer.eval()\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yFY-7KGiUarW"
      },
      "outputs": [],
      "source": [
        "# @title Generate\n",
        "prompt = \"What is the heaviest element?\" # @param {type:\"string\"}\n",
        "max_gen_len = 1024  # @param {type:\"integer\"}\n",
        "tokens = tokenizer.encode(prompt, bos=True, eos=False)\n",
        "\n",
        "# Create the embeddings of the initial prompt.\n",
        "x = embedding_layer(torch.tensor(tokens)).detach().cpu().numpy()\n",
        "x = np.expand_dims(x, axis=0).astype(data_type)\n",
        "\n",
        "# Create the attention mask.\n",
        "attn_mask = -10000.0 * torch.triu(\n",
        "    torch.ones(attn_mask_shape), diagonal=1\n",
        ").cpu().detach().numpy().astype(data_type)\n",
        "\n",
        "# Create the K and V caches.\n",
        "head_dim = int(hidden_size / n_heads)\n",
        "k_cache = np.zeros([1, n_layers, max_seq_len, n_heads, head_dim], dtype=data_type)\n",
        "v_cache = np.zeros([1, n_layers, max_seq_len, n_heads, head_dim], dtype=data_type)\n",
        "\n",
        "# Iteratively generate tokens.\n",
        "pos = np.array(0)\n",
        "output_tokens = []\n",
        "for idx in range(max_gen_len):\n",
        "    results = llm_session.run(\n",
        "        None,\n",
        "        {\n",
        "            \"x\": x,\n",
        "            \"attn_mask\": attn_mask,\n",
        "            \"k_cache\": k_cache[:, :, :pos],\n",
        "            \"v_cache\": v_cache[:, :, :pos],\n",
        "            \"pos\": pos.astype(np.int64),\n",
        "        },\n",
        "    )\n",
        "    logits, k_out, v_out = results[:3]\n",
        "\n",
        "    # Decide the next token using your preferred sampling strategy.\n",
        "    next_token = np.argmax(logits, axis=-1).astype(np.int64)\n",
        "    output_tokens.extend(next_token)\n",
        "\n",
        "    # Stop if/when we get an ENDOFTEXT token before reaching maximum sequence length\n",
        "    if next_token == tokenizer.eos_id:\n",
        "        break\n",
        "\n",
        "    # Update the cache\n",
        "    seq_len = x.shape[1]\n",
        "    k_cache[:, :, pos : pos + seq_len] = k_out\n",
        "    v_cache[:, :, pos : pos + seq_len] = v_out\n",
        "\n",
        "    # Update pos and x ready for the next round.\n",
        "    pos = np.array(int(pos) + seq_len, dtype=np.int64)\n",
        "    x = embedding_layer(torch.tensor(next_token)).unsqueeze(0)\n",
        "    x = x.cpu().detach().numpy().astype(data_type)\n",
        "\n",
        "output_str = tokenizer.decode(torch.tensor(output_tokens).tolist())\n",
        "print(output_str)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
